{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import label_to_y, word_embeddings, pad_x\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing\n",
    "\n",
    "\n",
    "load tokens with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"data/tokens_stopwords_removed_with_label_full.csv\")\n",
    "d = d.rename(columns={\"TOKENS_SW_RMED\": \"TOKENS\"})\n",
    "d = d[['SUBJECT_ID', 'HADM_ID', 'TOKENS', 'LABEL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['TOKENS'] = d['TOKENS'].str.split(',')\n",
    "d['LABEL'] = d['LABEL'].apply(lambda x: x.split(',') if isinstance(x, str) else x)\n",
    "d['LABEL'] = d['LABEL'].apply(lambda x: [x] if isinstance(x, float) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ad = d.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trunctate by max length of 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['Tokens_Tr'] = d['TOKENS'].apply(lambda x: x[:2500])\n",
    "d = d[['SUBJECT_ID', 'HADM_ID', 'Tokens_Tr', 'LABEL', 'TOKENS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter top 50 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = defaultdict(int)\n",
    "set_labels = set()\n",
    "\n",
    "for i in range(num_ad):\n",
    "    labels = d.iloc[i, 3]\n",
    "    for l in labels:\n",
    "        set_labels.add(l)\n",
    "        count_labels[l] += 1\n",
    "\n",
    "sorted_labels = sorted(count_labels.items(), key=lambda x:x[1], reverse=True)\n",
    "top_50_labels = sorted_labels[:50]\n",
    "top_50_labels = [x[0] for x in top_50_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_label(x):\n",
    "    result = []\n",
    "    for xx in x:\n",
    "        if xx in top_50_labels:\n",
    "            result.append(xx)\n",
    "    if result == []:\n",
    "        return False\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['Filtered_Label'] = d['LABEL'].apply(filter_label)\n",
    "filtered_df = d[d['Filtered_Label'] != False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dict of labels map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_ad_filtered = filtered_df.shape[0]\n",
    "set_labels = set()\n",
    "\n",
    "for i in range(num_ad_filtered):\n",
    "    labels = filtered_df.iloc[i, 5]\n",
    "    for l in labels:\n",
    "        set_labels.add(l)\n",
    "num_labels = len(set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_labels = dict(zip(set_labels, range(num_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split into train/test/valid\n",
    "\n",
    "load lists of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8066, 1573, 1729)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_train = pd.read_csv(\"data/train_50_hadm_ids.csv\", header=None)[0].to_list()\n",
    "id_valid = pd.read_csv(\"data/dev_50_hadm_ids.csv\", header=None)[0].to_list()\n",
    "id_test = pd.read_csv(\"data/test_50_hadm_ids.csv\", header=None)[0].to_list()\n",
    "\n",
    "len(id_train),len(id_valid),len(id_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = filtered_df[filtered_df['HADM_ID'].isin(id_train)]\n",
    "df_valid = filtered_df[filtered_df['HADM_ID'].isin(id_valid)]\n",
    "df_test = filtered_df[filtered_df['HADM_ID'].isin(id_test)]\n",
    "\n",
    "df_all = pd.concat([df_train, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ad = df_all.shape[0]\n",
    "set_vocab = set()\n",
    "\n",
    "for i in range(num_ad):\n",
    "    tks = df_all.iloc[i, 2]\n",
    "    for t in tks:\n",
    "        set_vocab.add(t)\n",
    "\n",
    "num_vocab = len(set_vocab)\n",
    "dict_vocab = dict(zip(set_vocab, range(num_vocab)))\n",
    "dict_vocab = {k : (v+1) for k,v in dict_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map tokens to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Ids'] = df_all['Tokens_Tr'].apply(lambda x: [dict_vocab[t] for t in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to local file for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.to_csv(\"data/for_training/data_for_nn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_all[df_all['HADM_ID'].isin(id_train)]\n",
    "df_valid = df_all[df_all['HADM_ID'].isin(id_valid)]\n",
    "df_test = df_all[df_all['HADM_ID'].isin(id_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['Ids'].to_numpy()\n",
    "X_valid = df_valid['Ids'].to_numpy()\n",
    "X_test = df_test['Ids'].to_numpy()\n",
    "\n",
    "y_train = label_to_y(df_train, dict_labels)\n",
    "y_valid = label_to_y(df_valid, dict_labels)\n",
    "y_test = label_to_y(df_test, dict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7061,), (1369,), (1504,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = max([len(_) for _ in df_all['Tokens_Tr']])  # 2500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_x(X_train, max_tokens)\n",
    "X_valid_pad = pad_x(X_valid, max_tokens)\n",
    "X_test_pad = pad_x(X_test, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform to input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_pad)\n",
    "X_valid = torch.from_numpy(X_valid_pad)\n",
    "X_test = torch.from_numpy(X_test_pad)\n",
    "\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_valid = torch.from_numpy(y_valid)\n",
    "y_test = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_train, 'data/for_training/X_train50.pt')\n",
    "torch.save(X_valid, 'data/for_training/X_valid50.pt')\n",
    "torch.save(X_test, 'data/for_training/X_test50.pt')\n",
    "\n",
    "torch.save(y_train, 'data/for_training/y_train50.pt')\n",
    "torch.save(y_valid, 'data/for_training/y_valid50.pt')\n",
    "torch.save(y_test, 'data/for_training/y_test50.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordEmbeddingsMatrix = utils.word_embeddings(df_all['Tokens_Tr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ICD Code Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_map_desc(codes):\n",
    "    desc_dict = {}\n",
    "    with open('data/ICD_descriptions.txt', 'r') as f:\n",
    "        for i, row in enumerate(f):\n",
    "            row = row.strip().split()\n",
    "            cd = row[0]\n",
    "            if cd in codes:\n",
    "                desc_dict[cd] = ' '.join(row[1:])\n",
    "                \n",
    "    return desc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data = code_map_desc(top_50_labels)\n",
    "desc_data = {dict_labels[k]:v for k,v in desc_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\d*[a-zA-Z]+\\d*')\n",
    "desc_data = {k:tokenizer.tokenize(v) for k,v in desc_data.items()}\n",
    "desc_data = {k:[dict_vocab[x] if x in dict_vocab.keys() else 0 for x in v] for k,v in desc_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_key = sorted(desc_data)\n",
    "desc_data = {k:desc_data[k] for k in sorted_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_data = {k:[str(x) for x in v] for k,v in desc_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/desc_data.csv', 'w') as csv_file:  \n",
    "#     writer = csv.writer(csv_file)\n",
    "#     for key, value in desc_data.items():\n",
    "#         writer.writerow([key, ' '.join(value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
