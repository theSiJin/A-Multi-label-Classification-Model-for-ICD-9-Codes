{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialize environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, size, collect_list, concat_ws, flatten\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_path = 'mimic-iii-clinical-database-1.4'\n",
    "note_file_path = '{}/NOTEEVENTS.csv'.format(mimic_path)\n",
    "diag_file_path = '{}/DIAGNOSES_ICD.csv'.format(mimic_path)\n",
    "proc_file_path = '{}/PROCEDURES_ICD.csv'.format(mimic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiag = spark.read.option('header', True).csv(diag_file_path)\n",
    "dfproc = spark.read.option('header', True).csv(proc_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Code Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add period to ICD codes to avoid confliction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_period(x):\n",
    "    if x is None:\n",
    "        return ''\n",
    "    \n",
    "    if x[0] == 'E':\n",
    "        if len(x) > 4:\n",
    "            x = x[:4] + '.' + x[4:]\n",
    "    else:\n",
    "        if len(x) > 3:\n",
    "            x = x[:3] + '.' + x[3:]\n",
    "\n",
    "    return x\n",
    "udf_diag = udf(lambda x: add_period(x))\n",
    "            \n",
    "dfdiag = dfdiag.withColumn('ICD9_CODE_P', udf_diag('ICD9_CODE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiag = dfdiag.na.drop()\n",
    "dfdiag = dfdiag.drop('ICD9_CODE').withColumnRenamed('ICD9_CODE_P', 'ICD9_CODE')\n",
    "\n",
    "udf_proc = udf(lambda x: x[:2]+'.'+x[2:])\n",
    "dfproc = dfproc.withColumn('ICD9_CODE', udf_proc('ICD9_CODE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine code in diag and proc into a single df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcode = dfdiag.union(dfproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique ICD-9 codes in diag:  6984\n",
      "Count of unique ICD-9 codes in proc:  2032\n",
      "Count of unique ICD-9 codes in total:  9016\n"
     ]
    }
   ],
   "source": [
    "count_diag_code = dfdiag.select('ICD9_CODE').distinct().count()\n",
    "count_proc_code = dfproc.select('ICD9_CODE').distinct().count()\n",
    "count_total_code = dfcode.select('ICD9_CODE').distinct().count()\n",
    "\n",
    "print(\"Count of unique ICD-9 codes in diag: \", count_diag_code)\n",
    "print(\"Count of unique ICD-9 codes in proc: \", count_proc_code)\n",
    "print(\"Count of unique ICD-9 codes in total: \", count_total_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select only top 50 labels with most occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_code = dfcode.select('ICD9_CODE').groupBy('ICD9_CODE').count()\n",
    "df = count_code.sort(col('count').desc()).toPandas()\n",
    "top_50_labels = df.iloc[:50, 0].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load notes text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ('SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'TEXT')\n",
    "dfnote = spark.read.option(\"header\",True).option(\"multiLine\", True).csv(note_file_path)\n",
    "dfdisc = dfnote.where(dfnote.CATEGORY == 'Discharge summary').select(*cols)\n",
    "dfdisc = dfdisc.drop('CHARTTIME', 'CHARTDATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean texts\n",
    "\n",
    "- lowercase\n",
    "- tokenize\n",
    "- remove numeric tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"TEXT\", outputCol=\"TOKENS\", pattern=r\"\\d*[a-zA-Z]+\\d*\", gaps=False)\n",
    "dfdisc = regexTokenizer.transform(dfdisc).drop('TEXT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter text based on top 50 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcode_filter = dfcode.filter(dfcode.ICD9_CODE.isin(top_50_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Join codes with texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate df by patient and SUBJECT_ID and HADM_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfcode_agg = dfcode_filter.groupby('SUBJECT_ID', 'HADM_ID').agg(collect_list('ICD9_CODE').alias('LABEL'))\n",
    "\n",
    "dfdisc_agg = dfdisc.groupby('HADM_ID').agg(collect_list('TOKENS').alias('TOKENS'))\n",
    "dfdisc_agg = dfdisc_agg.select('HADM_ID', flatten('TOKENS').alias('TOKENS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join the two dfs (codes & texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = (dfcode_agg.SUBJECT_ID, dfcode_agg.HADM_ID, 'TOKENS', 'LABEL')\n",
    "\n",
    "df = dfdisc_agg.join(dfcode_agg, dfcode_agg.HADM_ID==dfdisc_agg.HADM_ID, how='left')\n",
    "df = df.select(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol='TOKENS', outputCol='TOKENS_SW_RMED')\n",
    "df = swr.transform(df)\n",
    "df = df.drop('TOKENS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.withColumn('TOKENS_SW_RMED', concat_ws(',', 'TOKENS_SW_RMED'))\\\n",
    "#     .withColumn('LABEL', concat_ws(',', 'LABEL'))\\\n",
    "#     .coalesce(1)\\\n",
    "#     .write.option('header', True)\\\n",
    "#     .csv('data/tokens_stopwords_removed__with_label_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
